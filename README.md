# Nyströmformer Seq2Seq

An attempt to create a seq2seq model with Nyström approximation of the attention mechanism, based on the paper [_Nyströmformer: A Nyström-based Algorithm for Approximating Self-Attention_](https://arxiv.org/abs/2102.03902) and its [implementation](https://github.com/lucidrains/nystrom-attention) by @lucidrains.

# Citations
```
@misc{xiong2021nystromformer,
    title   = {Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention},
    author  = {Yunyang Xiong and Zhanpeng Zeng and Rudrasis Chakraborty and Mingxing Tan and Glenn Fung and Yin Li and Vikas Singh},
    year    = {2021},
    eprint  = {2102.03902},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}
```
